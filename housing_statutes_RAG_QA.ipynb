{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188ab73-2d18-4828-a967-0b608887dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook can be used to generate results for HousingStatutesQA in the project report\n",
    "# Luka Rozgic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14707455-2c4c-49ba-bd6d-fec8a1c9a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Set up paths\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "import gc\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291b42d-ba45-4063-b883-899c385099d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Set up paths\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "parent_path = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(parent_path, \"data\")\n",
    "code_path = os.path.join(parent_path, \"code\") # notebook is in code\n",
    "vectorstore_path = os.path.join(data_path, 'statute_vectorstore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e71e0-5e7c-4d64-a092-43a48a00c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Global Variables\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Embedding models with corresponding max input sequence lenghts and batch size for batch encoding (optimizad for 24GB GPU VRAM)\n",
    "embedding_models = [{\"model name\": \"intfloat/e5-base-v2\",       \"max sentence length\": 512,   \"model id\": \"e5_base_v2\",  \"batch size\": 16},\n",
    "                    {\"model name\": \"intfloat/e5-large-v2\",      \"max sentence length\": 512,   \"model id\": \"e5_large_v2\", \"batch size\": 16},\n",
    "                    {\"model name\": \"Qwen/Qwen3-Embedding-0.6B\", \"max sentence length\": 8192,  \"model id\": \"qwen3_0p6b\",  \"batch size\": 8}]\n",
    "\n",
    "# Queries for QA (question-only, expansion-only, question + expansion)\n",
    "retrieval_query_types = ['q', 'claude3sonnet', 'q_claude3sonnet', 'qwen2p57b', 'q_qwen2p57b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5a161-6353-43ed-8420-a6ea525ac063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Class for FEISS embedding and query to support L2 normalization and cosine similarity calculation\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "class CosineQueryEmbeddings:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        embedding = self.model.encode([text])[0]\n",
    "        return (embedding / np.linalg.norm(embedding)).tolist()\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts, normalize_embeddings=True).tolist()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122cca8-d05b-42cd-9ab1-b96164a04d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Functions\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Create FAISS vector store from documents\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def create_vectorstore(embedding_models, statutes_list, to_save = True, use_GPUs = 4):\n",
    "\n",
    "    # Create statute texts for embedding and metadata to be added to the vector store\n",
    "    statuteTexts = []\n",
    "    statuteMetadata = []\n",
    "    for statute in statutes_list:\n",
    "        statuteTexts.append(statute['text'])\n",
    "        statuteMetadata.append({\"id\":statute['idx'], \"citation\": statute['citation'].lower(), \"state\": statute['state'].lower()})\n",
    "\n",
    "    for model_spec_dict in embedding_models:\n",
    "\n",
    "        # Clean GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        embedding_model_name = model_spec_dict[\"model name\"]\n",
    "        model = SentenceTransformer(embedding_model_name)\n",
    "        model.max_seq_length = model_spec_dict[\"max sentence length\"]\n",
    "        # Reduce footprint of Qwen model to increase eficiency\n",
    "        if model_spec_dict['model id'] == \"qwen3_0p6b\": \n",
    "            model.half()\n",
    "\n",
    "        # Encode statutes (multi-GPU) \n",
    "        print('Embedding statutes using %s ... ' %(embedding_model_name))\n",
    "        # Encode statutes (multi-GPU) \n",
    "\n",
    "        start_time = time.time()\n",
    "        embeddings = model.encode(\n",
    "            statuteTexts, \n",
    "            batch_size=model_spec_dict[\"batch size\"],\n",
    "            device=['cuda:'+ str(cuda_id) for cuda_id in range(0,use_GPUs)],\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        print('Statutes embedded using %s in %.2f seconds' % (embedding_model_name, time.time()-start_time))\n",
    "\n",
    "        # Create vectorstore with calculated embeddings\n",
    "        cosine_embeddings = CosineQueryEmbeddings(model)\n",
    "        vectorstore = FAISS.from_embeddings(\n",
    "            text_embeddings=list(zip(statuteTexts, embeddings)),\n",
    "            embedding=cosine_embeddings,\n",
    "            metadatas=statuteMetadata\n",
    "        )\n",
    "        if to_save:\n",
    "            vectorstore.save_local(os.path.join(vectorstore_path, model_spec_dict[\"model id\"]))\n",
    "    # Clean GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Single query expansion using Claude 3 Sonnet\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def call_claude_with_retry(question, state, max_retries=3):\n",
    "    bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "    prompt = f\"\"\"Consider the housing statute for {state} in the year 2021. The question given in \"Question:\" is a legal question about housing and eviction law in {state}. Provide applicable legal rule in \"Rule:\". If you do not know the state law, provide governing rules that address the question under typical eviction law.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Rule:\"\"\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = bedrock.invoke_model(\n",
    "                modelId='anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "                body=json.dumps({\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                    \"max_tokens\": 500,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "                })\n",
    "            )\n",
    "            result = json.loads(response['body'].read())\n",
    "            return result['content'][0]['text']\n",
    "        except Exception as e:\n",
    "            if \"throttling\" in str(e).lower() or \"rate\" in str(e).lower():\n",
    "                wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                return f\"Error: {str(e)}\"\n",
    "    return \"Failed after retries\"\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Multi-thread solution to make multiple inference calls to Claude 3 Sonnet and generate query expansions, progress tracking across batches\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def query_expansions_claude_with_batch_progress(questions, max_workers=5, to_save=True):\n",
    "    results = []\n",
    "    completed_count = 0\n",
    "\n",
    "    # Calculate total batches of 100\n",
    "    total_batches = (len(questions) + 99) // 100\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_question = {\n",
    "            executor.submit(call_claude_with_retry, q['question'], q['state']): q \n",
    "            for q in questions\n",
    "        }\n",
    "\n",
    "        # Progress bar for batches of 100\n",
    "        with tqdm(total=total_batches, desc=\"Processing batches of 100\") as pbar:\n",
    "            for future in as_completed(future_to_question):\n",
    "                question = future_to_question[future]\n",
    "                try:\n",
    "                    rule = future.result()\n",
    "                    results.append({\n",
    "                        'idx': question['idx'],\n",
    "                        'question': question['question'],\n",
    "                        'state': question['state'],\n",
    "                        'rule': rule\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    results.append({\n",
    "                        'idx': question['idx'],\n",
    "                        'question': question['question'],\n",
    "                        'state': question['state'],\n",
    "                        'rule': f\"Error: {str(e)}\"\n",
    "                    })\n",
    "\n",
    "                completed_count += 1\n",
    "\n",
    "                # Update progress bar every 100 completions\n",
    "                if completed_count % 100 == 0:\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix(completed=completed_count)\n",
    "\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            # Update for remaining items\n",
    "            if completed_count % 100 != 0:\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(completed=completed_count)\n",
    "\n",
    "    if to_save:\n",
    "        with open(os.path.join(store_dir,'claude3sonnet_generated_expansions.json'), 'w') as f:\n",
    "            json.dump(structured_reasoning_results, f, indent=2)\n",
    "\n",
    "    return results\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generate all query expansion prompts upfront as for Qwen 2.5 7B Instruct we want to generate expansions in batches for speed\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def generate_query_expansion_prompts(questions):\n",
    "    \"\"\"Generate prompts from questions \"\"\"\n",
    "    prompts_with_metadata = []\n",
    "\n",
    "    for q in questions:\n",
    "\n",
    "        state = q['state']\n",
    "        question = q['question']\n",
    "        idx = q['idx']\n",
    "\n",
    "        prompt = f\"\"\"Consider the housing statute for {state} in the year 2021. The question given in \"Question:\" is a legal question about housing and eviction law in {state}. Provide applicable legal rule in \"Rule:\". If you do not know the state law, provide governing rules that address the question under typical eviction law.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Rule:\"\"\"\n",
    "        prompts_with_metadata.append({\n",
    "            'idx': idx,\n",
    "            'question': question,\n",
    "            'state': state,\n",
    "            'prompt': prompt\n",
    "        })\n",
    "    return prompts_with_metadata\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generate all query expansions using Qwen 2.5 7B Instruct and VLLM\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def generate_query_expansions(prompts_with_metadata, llm, sampling_params, batch_size=32, to_save=True):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts_with_metadata), batch_size), desc=\"Processing batches\"):\n",
    "\n",
    "        batch_prompts   = [pwm['prompt'] for pwm in prompts_with_metadata[i:i+batch_size]]\n",
    "        batch_idxs      = [pwm['idx'] for pwm in prompts_with_metadata[i:i+batch_size]]\n",
    "        batch_states    = [pwm['state'] for pwm in prompts_with_metadata[i:i+batch_size]]\n",
    "        batch_questions = [pwm['question'] for pwm in prompts_with_metadata[i:i+batch_size]]\n",
    "\n",
    "        outputs = llm.generate(batch_prompts, sampling_params)\n",
    "\n",
    "        cnt = 0\n",
    "        for output in outputs:\n",
    "            rule = output.outputs[0].text\n",
    "            results.append({\n",
    "                        'idx': batch_idxs[cnt],\n",
    "                        'question': batch_questions[cnt],\n",
    "                        'state': batch_states[cnt],\n",
    "                        'rule': rule\n",
    "            })\n",
    "            cnt+=1\n",
    "    if to_save:\n",
    "        with open(os.path.join(data_path,'qwen2p57b_generated_expansions.json'), 'w') as f:\n",
    "            json.dump(structured_reasoning_results, f, indent=2)\n",
    "    return results\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generate all queries to be used for QA (question only, expansion-only and question+expansion)\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def generate_retrieval_query_list(retrieval_query_type, questions_list):\n",
    "    \"\"\"Generate queries for different retrieval query types\"\"\"\n",
    "\n",
    "    if retrieval_query_type == 'q':\n",
    "        # Just use the question\n",
    "        return [{**q, 'query': q['question']} for q in questions_list]\n",
    "\n",
    "    elif retrieval_query_type == 'claude3sonnet':\n",
    "        # Load Claude 3 Sonnet rules\n",
    "        with open(os.path.join(data_path, 'claude3sonnet_generated_expansions.json'), 'r') as f:\n",
    "            claude_rules = json.load(f)\n",
    "        claude_dict = {item['idx']: item['rule'] for item in claude_rules}\n",
    "        return [{**q, 'query': claude_dict.get(q['idx'], '')} for q in questions_list]\n",
    "\n",
    "    elif retrieval_query_type == 'qwen2p57b':\n",
    "        # Load Qwen 2.5 7B rules\n",
    "        with open(os.path.join(data_path, 'qwen2p57b_generated_expansions.json'), 'r') as f:\n",
    "            qwen_rules = json.load(f)\n",
    "        qwen_dict = {item['idx']: item['rule'] for item in qwen_rules}\n",
    "        return [{**q, 'query': qwen_dict.get(q['idx'], '')} for q in questions_list]\n",
    "\n",
    "    elif retrieval_query_type == 'q_claude3sonnet':\n",
    "        # Concatenate question + Claude rules\n",
    "        with open(os.path.join(data_path, 'claude3sonnet_generated_expansions.json'), 'r') as f:\n",
    "            claude_rules = json.load(f)\n",
    "        claude_dict = {item['idx']: item['rule'] for item in claude_rules}\n",
    "        return [{**q, 'query': q['question'] + ' ' + claude_dict.get(q['idx'], '')} for q in questions_list]\n",
    "\n",
    "    elif retrieval_query_type == 'q_qwen2p57b':\n",
    "        # Concatenate question + Qwen rules\n",
    "        with open(os.path.join(data_path, 'qwen2p57b_generated_expansions.json'), 'r') as f:\n",
    "            qwen_rules = json.load(f)\n",
    "        qwen_dict = {item['idx']: item['rule'] for item in qwen_rules}\n",
    "        return [{**q, 'query': q['question'] + ' ' + qwen_dict.get(q['idx'], '')} for q in questions_list]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown retrieval_query_type: {retrieval_query_type}\")\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Collect all retrieves (up to top_to_store) for all combinations of embedding models and queries, output contains ground truths as well\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def generate_recall_results(embedding_models, retrieval_query_types, vectorstore_path, to_save=True, top_to_store=100, top_to_search=10000):\n",
    "    recall_results = dict()\n",
    "    for embedding_model in embedding_models:\n",
    "\n",
    "        recall_results[embedding_model[\"model id\"]] = dict()\n",
    "\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # Load model to be used for querying (sentence transformers)\n",
    "        model = SentenceTransformer(embedding_model[\"model name\"])\n",
    "        model.max_seq_length = model_spec_dict[\"max sentence length\"] \n",
    "        if model_spec_dict['model id'] == \"qwen3_0p6b\":\n",
    "            model.half()\n",
    "\n",
    "        # Load corresponding housing statutes vector store\n",
    "        cosine_embeddings = CosineQueryEmbeddings(model)\n",
    "        vectorstore = FAISS.load_local(\n",
    "            os.path.join(vectorstore_path, embedding_model[\"model id\"]), \n",
    "            cosine_embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "        for retrieval_query_type in retrieval_query_types:\n",
    "            recall_results[embedding_model[\"model id\"]][retrieval_query_type] = []\n",
    "            query_list = generate_retrieval_query_list(retrieval_query_type, questions_list)\n",
    "\n",
    "            # Filter valid queries and batch encode once\n",
    "            valid_queries = [q for q in query_list if q['statutes']]\n",
    "            questions_text = [q['query'] for q in valid_queries]\n",
    "\n",
    "            query_embeddings = model.encode(\n",
    "                questions_text,\n",
    "                batch_size=embedding_model[\"batch size\"],\n",
    "                device=['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3'],\n",
    "                normalize_embeddings=True,\n",
    "                show_progress_bar=True\n",
    "            )\n",
    "\n",
    "            # Single batch search for all queries\n",
    "            scores, indices = vectorstore.index.search(query_embeddings, top_to_search)\n",
    "\n",
    "            # Populate retrieval store\n",
    "            # retrievals[model id][retrieval_query_type] = [{'idx': question id, 'state': state, 'golden statute idx': {},  'top_k_statute_idxs': [], 'top_k_statute_scores': []} \n",
    "\n",
    "            for query_idx, query in enumerate(tqdm(valid_queries, desc=\"Processing queries\")):\n",
    "\n",
    "                # Find top 100 in the corresponding state\n",
    "                retrieved_statute_idxs   = []\n",
    "                retrieved_statute_scores = []\n",
    "                for score, idx in zip(scores[query_idx], indices[query_idx]):\n",
    "                    if idx != -1:\n",
    "                        doc_id = vectorstore.index_to_docstore_id[idx]\n",
    "                        doc = vectorstore.docstore.search(doc_id)\n",
    "\n",
    "                        if doc.metadata['state'] == query['state'].lower():\n",
    "                            retrieved_statute_idxs.append(doc.metadata['id'])\n",
    "                            retrieved_statute_scores.append(score)\n",
    "                            if len(retrieved_statute_idxs) >= top_to_store:\n",
    "                                break\n",
    "\n",
    "                recall_results[embedding_model[\"model id\"]][retrieval_query_type].append({\n",
    "                                                                                        'idx': query['idx'], \n",
    "                                                                                        'state': query['state'].lower(), \n",
    "                                                                                        'golden statute idx': {s['statute_idx'] for s in query['statutes']},\n",
    "                                                                                        'retrieved_idxs': retrieved_statute_idxs, \n",
    "                                                                                        'retrieved_scores': retrieved_statute_scores})\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        if to_save:\n",
    "            with open(os.path.join(data_path, 'recall_results_all.pkl'), 'wb') as f:\n",
    "                pickle.dump(recall_results, f)\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Calculate recalls from collected retrieval results and ground truths\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def calculate_recall_at_k_with_ci(recall_results, selected_query_idxs, k, n_bootstrap=1000, confidence=0.95, to_print_table=True):\n",
    "\n",
    "    recall_scores = {}\n",
    "\n",
    "    for model_id in recall_results:\n",
    "        recall_scores[model_id] = {}\n",
    "        for query_type in recall_results[model_id]:\n",
    "            results = recall_results[model_id][query_type]\n",
    "            #n_queries = len(results)\n",
    "            n_queries = 0\n",
    "            for result in results:\n",
    "                if result['idx'] in selected_query_idxs:\n",
    "                    n_queries+=1\n",
    "\n",
    "            # Calculate individual query hits\n",
    "            #hits = []\n",
    "            #for result in results:\n",
    "            #    golden_idxs = result['golden statute idx']\n",
    "            #    retrieved_idxs = set(result['retrieved_idxs'][:k])\n",
    "            #    hits.append(1 if golden_idxs.intersection(retrieved_idxs) else 0)\n",
    "\n",
    "            hits = []\n",
    "            for result in results:\n",
    "                if result['idx'] in selected_query_idxs:\n",
    "                    golden_idxs = result['golden statute idx']\n",
    "                    retrieved_idxs = set(result['retrieved_idxs'][:k])\n",
    "                    hits.append(1 if golden_idxs.intersection(retrieved_idxs) else 0)            \n",
    "\n",
    "            # Bootstrap sampling\n",
    "            bootstrap_scores = []\n",
    "            for _ in range(n_bootstrap):\n",
    "                sample_hits = np.random.choice(hits, size=n_queries, replace=True)\n",
    "                bootstrap_scores.append(np.mean(sample_hits))\n",
    "\n",
    "            # Calculate confidence interval\n",
    "            alpha = 1 - confidence\n",
    "            lower_percentile = (alpha/2) * 100\n",
    "            upper_percentile = (1 - alpha/2) * 100\n",
    "\n",
    "            recall_scores[model_id][query_type] = {\n",
    "                'recall': np.mean(hits),\n",
    "                'ci_lower': np.percentile(bootstrap_scores, lower_percentile),\n",
    "                'ci_upper': np.percentile(bootstrap_scores, upper_percentile)\n",
    "            }\n",
    "\n",
    "    # Print table\n",
    "    if to_print_table:\n",
    "        table_data = []\n",
    "        for model_id in recall_scores:\n",
    "            for query_type in recall_scores[model_id]:\n",
    "                scores = recall_scores[model_id][query_type]\n",
    "                table_data.append({\n",
    "                    'Model': model_id,\n",
    "                    'Query Type': query_type,\n",
    "                    f'Recall@{k}': f\"{scores['recall']:.3f}\",\n",
    "                    'CI Range': f\"[{scores['ci_lower']:.3f}, {scores['ci_upper']:.3f}]\"\n",
    "                })\n",
    "        df = pd.DataFrame(table_data)\n",
    "        print(df.to_string(index=False))\n",
    "\n",
    "    return recall_scores\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Calculate recall@k per state for all states corresponding to question-statute pairs\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def calculate_recall_per_state_with_ci(recall_results, selected_query_idxs, k, model_id, query_type, n_bootstrap=1000, confidence=0.95):\n",
    "    import numpy as np\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Group results by state\n",
    "    state_results = defaultdict(list)\n",
    "    for result in recall_results[model_id][query_type]:\n",
    "        if result['idx'] in selected_query_idxs:\n",
    "            state_results[result['state']].append(result)\n",
    "\n",
    "    state_recalls = {}\n",
    "    for state, results in state_results.items():\n",
    "        # Calculate individual query hits\n",
    "        hits = []\n",
    "        for result in results:\n",
    "            golden_idxs = result['golden statute idx']\n",
    "            retrieved_idxs = set(result['retrieved_idxs'][:k])\n",
    "            hits.append(1 if golden_idxs.intersection(retrieved_idxs) else 0)\n",
    "\n",
    "        if len(hits) == 0:\n",
    "            continue\n",
    "\n",
    "        # Bootstrap sampling\n",
    "        bootstrap_scores = []\n",
    "        for _ in range(n_bootstrap):\n",
    "            sample_hits = np.random.choice(hits, size=len(hits), replace=True)\n",
    "            bootstrap_scores.append(np.mean(sample_hits))\n",
    "\n",
    "        # Calculate confidence interval\n",
    "        alpha = 1 - confidence\n",
    "        lower_percentile = (alpha/2) * 100\n",
    "        upper_percentile = (1 - alpha/2) * 100\n",
    "\n",
    "        state_recalls[state] = {\n",
    "            'recall': np.mean(hits),\n",
    "            'ci_lower': np.percentile(bootstrap_scores, lower_percentile),\n",
    "            'ci_upper': np.percentile(bootstrap_scores, upper_percentile),\n",
    "            'n_queries': len(hits)\n",
    "        }\n",
    "    return state_recalls\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Plot recalls per state in increasing order with confidence intervals\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def plot_recall_by_state(state_recalls, k, model_id, query_type, statutes_list):\n",
    "\n",
    "    # Count statutes per state\n",
    "    statute_counts = Counter(statute['state'].lower() for statute in statutes_list)\n",
    "\n",
    "    # Sort states by recall\n",
    "    sorted_states = sorted(state_recalls.items(), key=lambda x: x[1]['recall'])\n",
    "\n",
    "    states = [item[0] for item in sorted_states]\n",
    "    recalls = [item[1]['recall'] for item in sorted_states]\n",
    "    ci_lower = [item[1]['ci_lower'] for item in sorted_states]\n",
    "    ci_upper = [item[1]['ci_upper'] for item in sorted_states]\n",
    "    n_queries = [item[1]['n_queries'] for item in sorted_states]\n",
    "    n_statutes = [statute_counts[state] for state in states]\n",
    "\n",
    "    # Calculate error bars\n",
    "    yerr_lower = [recalls[i] - ci_lower[i] for i in range(len(recalls))]\n",
    "    yerr_upper = [ci_upper[i] - recalls[i] for i in range(len(recalls))]\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.errorbar(range(len(states)), recalls, yerr=[yerr_lower, yerr_upper], \n",
    "                 fmt='o', capsize=5, capthick=2)\n",
    "\n",
    "    # Set x-axis labels with state names\n",
    "    plt.xticks(range(len(states)), states, rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "    # Add question and statute counts as vertical text\n",
    "    for i, (recall, n_q, n_s) in enumerate(zip(recalls, n_queries, n_statutes)):\n",
    "        plt.text(i + 0.25, recall + 0.02, f'q={n_q}\\ns={n_s}', ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "\n",
    "    plt.ylabel(f'Recall@{k}')\n",
    "    plt.xlabel('State')\n",
    "    plt.title(f'Recall@{k} by State - {model_id} ({query_type})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generate all prompts for question answering (query, query + golden passage (oracle aka. upper bound), query + top-10 retrieved statutes\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def generate_prompts(questions, retrieved_statutes=None, statute_lookup=None):\n",
    "    \"\"\"Generate prompts with three options: question only, golden context, retrieved context\"\"\"\n",
    "    prompts = {\n",
    "        'question_only': [],\n",
    "        'golden_context': [],\n",
    "        'retrieved_context': []\n",
    "    }\n",
    "    ground_truths = []\n",
    "\n",
    "    for i, q in enumerate(questions):\n",
    "        # Option 1: Question only\n",
    "        prompt_q_only = f\"Question: {q['question']}\\n\\nAnswer with only 'Yes' or 'No':\"\n",
    "        prompts['question_only'].append(prompt_q_only)\n",
    "\n",
    "        # Option 2: Question + golden statute excerpts\n",
    "        golden_excerpts = [s['excerpt'] for s in q['statutes']]\n",
    "        golden_context = \"\\n\".join(golden_excerpts)\n",
    "        prompt_golden = f\"Context: {golden_context}\\n\\nQuestion: {q['question']}\\n\\nAnswer with only 'Yes' or 'No':\"\n",
    "        prompts['golden_context'].append(prompt_golden)\n",
    "\n",
    "        # Option 3: Question + top 10 retrieved statutes\n",
    "        if retrieved_statutes and statute_lookup:\n",
    "            top_10_idxs = retrieved_statutes[i]  # Top 10 retrieved statute indices\n",
    "            retrieved_texts = [statute_lookup[idx] for idx in top_10_idxs if idx in statute_lookup]\n",
    "            retrieved_context = \"\\n\".join(retrieved_texts)\n",
    "            prompt_retrieved = f\"Context: {retrieved_context}\\n\\nQuestion: {q['question']}\\n\\nAnswer with only 'Yes' or 'No':\"\n",
    "        else:\n",
    "            prompt_retrieved = prompt_q_only  # Fallback to question only\n",
    "        prompts['retrieved_context'].append(prompt_retrieved)\n",
    "\n",
    "        ground_truths.append(q['answer'])\n",
    "\n",
    "    return prompts, ground_truths\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generate QA answers with Qwen 2.5 7B Instruct using batch inference with VLLM\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def evaluate_qa_batch(prompts, ground_truths, llm, sampling_params, batch_size=32):\n",
    "    def extract_yes_no(text):\n",
    "        \"\"\"Fast yes/no extraction\"\"\"\n",
    "        text = text.lower()[:10]\n",
    "        return 'Yes' if 'yes' in text else 'No' if 'no' in text else 'Unknown'\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Processing batches\"):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "        outputs = llm.generate(batch_prompts, sampling_params)\n",
    "\n",
    "        for output in outputs:\n",
    "            prediction = extract_yes_no(output.outputs[0].text)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Calculate QA accuracy with confidence intervals\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def calculate_metrics_with_ci(predictions, ground_truths):\n",
    "    \"\"\"Calculate accuracy with confidence intervals\"\"\"\n",
    "    valid_pairs = [(p, g) for p, g in zip(predictions, ground_truths) if p != 'Unknown']\n",
    "    if not valid_pairs:\n",
    "        return {'accuracy': 0, 'total': len(predictions)}\n",
    "\n",
    "    valid_preds, valid_gts = zip(*valid_pairs)\n",
    "    correct = sum(p == g for p, g in zip(valid_preds, valid_gts))\n",
    "    total = len(valid_pairs)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Bootstrap confidence intervals\n",
    "    n_bootstrap = 1000\n",
    "    indices = np.random.choice(total, (n_bootstrap, total), replace=True)\n",
    "    bootstrap_accs = np.mean([[valid_preds[i] == valid_gts[i] for i in boot_indices] for boot_indices in indices], axis=1)\n",
    "\n",
    "    ci_lower = np.percentile(bootstrap_accs, 2.5)\n",
    "    ci_upper = np.percentile(bootstrap_accs, 97.5)\n",
    "    std = np.std(bootstrap_accs)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'std': std,\n",
    "        'ci_95': (ci_lower, ci_upper),\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'unknown': len(predictions) - total\n",
    "    }\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Is substring helper function to check if golden statute text matches statute database text with the same index\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def is_substring(str1, str2):\n",
    "    # Normalize: lowercase, keep only letters\n",
    "    norm_str1 = re.sub(r'[^a-z]', '', str1.lower())\n",
    "    norm_str2 = re.sub(r'[^a-z]', '', str2.lower())\n",
    "\n",
    "    # Trim norm_str1 from left to 70% of length\n",
    "    if len(norm_str1) > 20:\n",
    "        trim_length = int(len(norm_str1) * 0.5)\n",
    "        norm_str1 = norm_str1[-trim_length:]\n",
    "\n",
    "    return norm_str1 in norm_str2\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Identify questions that pass golden passage - statute matching criteria\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "def verify_question_statute_match(questions_list, statutes_list, print_per_state_quality = True, to_save=True):\n",
    "    unique_states = {q['state'].lower() for q in questions_list}\n",
    "\n",
    "    statute_texts = {statute['idx']: statute['text'] for statute in statutes_list}\n",
    "\n",
    "    state_match = dict()\n",
    "    for state in unique_states:\n",
    "        state_match[state] = []\n",
    "\n",
    "    selected_questions = []\n",
    "    for question in questions_list:\n",
    "        state = question['state'].lower()\n",
    "        question_statute_idxs = {s['statute_idx']: s['excerpt'] for s in question['statutes']}\n",
    "\n",
    "        _tmp_match = []\n",
    "        for idx, golden_text in question_statute_idxs.items():\n",
    "            if is_substring(golden_text, statute_texts[idx]):\n",
    "                _tmp_match.append(1)\n",
    "            else:\n",
    "                _tmp_match.append(0)\n",
    "        if not 0 in _tmp_match:\n",
    "            selected_questions.append(question)\n",
    "        state_match[state].extend(_tmp_match)\n",
    "\n",
    "    if print_per_state_quality:\n",
    "        # Calculate average match percentage per state\n",
    "        state_averages = []\n",
    "        for state, matches in state_match.items():\n",
    "            avg_matches = sum(matches) / len(matches) if matches else 0\n",
    "            state_averages.append((state, avg_matches))\n",
    "\n",
    "        # Sort by increasing percentages\n",
    "        state_averages.sort(key=lambda x: x[1])\n",
    "\n",
    "        for state, avg_matches in state_averages:\n",
    "            print(f\"{state}: {avg_matches:.3f}\")\n",
    "    if to_save:\n",
    "        with open(os.path.join(data_path, 'selected_questions_with_matching_statutes.pkl'), 'wb') as f:\n",
    "            pickle.dump(selected_questions, f)\n",
    "\n",
    "    return selected_questions\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6fcf7c-225d-46c1-8af6-ae11c230d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Load Housing Statutes Dataset\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "# To enable loading HousingStatute and BarExam datasets a specific older version of datasets is required\n",
    "!pip3 install -U datasets==2.14.0 \n",
    "!pip3 install --upgrade huggingface-hub==0.20.0\n",
    "!pip3 install fsspec==2023.9.2\n",
    "\n",
    "# Load Housing Statutes Dataset\n",
    "questions = load_dataset(\"reglab/housing_qa\", \"questions\", split=\"test\")\n",
    "statutes = load_dataset(\"reglab/housing_qa\", \"statutes\", split=\"corpus\")\n",
    "\n",
    "# Save Housing Statutes Dataset\n",
    "with open(os.path.join(data_path, \"housing_statutes_dataset\", \"questions.json\"), \"w\") as f:\n",
    "    json.dump(questions.to_list(), f, indent=2)\n",
    "\n",
    "with open(os.path.join(data_path, \"housing_statutes_dataset\", \"statutes.json\"), \"w\") as f:\n",
    "    json.dump(statutes.to_list(), f, indent=2)\n",
    "'''\n",
    "\n",
    "with open(os.path.join(data_path, \"housing_statutes_dataset\", \"questions.json\"), \"r\") as f:\n",
    "    questions_list = json.load(f)\n",
    "\n",
    "with open(os.path.join(data_path, \"housing_statutes_dataset\", \"statutes.json\"), \"r\") as f:\n",
    "    statutes_list = json.load(f)\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Create statute vectorstores with all embedding models\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "create_vectorstore(embedding_models, statutes_list, to_save = True, use_GPUs = 4)\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generate Clude 3 Sonnet query expansions\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "claude_query_expansions = query_expansions_claude_with_batch_progress(questions, max_workers=5)\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generate Qwen 2.5 7B Instruct query expansions\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Initialize VLLM with Qwen 2.5 7B Instruct\n",
    "llm = LLM(\n",
    "    model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    tensor_parallel_size=4,\n",
    "    gpu_memory_utilization=0.9\n",
    ")\n",
    "\n",
    "# Set Qwen 2.5 7B Instuct generation parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=512\n",
    ")\n",
    "prompts_with_metadata = generate_query_expansion_prompts(questions_list)\n",
    "qwen_query_expansions = generate_query_expansions(prompts_with_metadata, llm, sampling_params, batch_size=32)\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Verify match between question golden passage text and statute database text with matching statute index and select questions\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "selected_questions = verify_question_statute_match(questions_list, statutes_list, print_per_state_quality = True, to_save=True)\n",
    "selected_query_idxs = {q['idx']: True for q in selected_questions}\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Perform retrieval evaluation (overall)\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "recall_results = generate_recall_results(embedding_models, retrieval_query_types, vectorstore_path, to_save=True, top_to_store=100, top_to_search=10000)\n",
    "recall_scores = calculate_recall_at_k_with_ci(recall_results, selected_query_idxs, k, n_bootstrap=1000, confidence=0.95, to_print_table=True)\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Perform retrieval evaluation per state\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "model_id = \"e5_large_v2\" # embedding model used\n",
    "query_type = 'q' # original queries only\n",
    "state_recalls = calculate_recall_per_state_with_ci(recall_results, selected_query_idxs, k, model_id, query_type, n_bootstrap=1000, confidence=0.95)\n",
    "plot_recall_by_state(state_recalls, k, model_id, query_type, statutes_list)\n",
    "\n",
    "model_id = \"e5_large_v2\" # embedding model used\n",
    "query_type = 'q_claude3sonnet' # original query + Claude 3 Sonnet expansion\n",
    "state_recalls = calculate_recall_per_state_with_ci(recall_results, selected_query_idxs, k, model_id, query_type, n_bootstrap=1000, confidence=0.95)\n",
    "plot_recall_by_state(state_recalls, k, model_id, query_type, statutes_list)\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Perform QA inference with different contexts (question, golden passage + question, retrieved statutes + question) and calcualte performance\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------\n",
    "prompts, ground_truths = generate_prompts(questions, retrieved_statutes=None, statute_lookup=None)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "predictions = evaluate_qa_batch(prompts['question_only'], ground_truths, llm, sampling_params, batch_size=32)\n",
    "results = calculate_metrics_with_ci(predictions, ground_truths)\n",
    "print(f\"Accuracy: {results['accuracy']:.3f}\")\n",
    "print(f\"Standard Deviation: {results['std']:.3f}\")\n",
    "print(f\"95% CI: ({results['ci_95'][0]:.3f}, {results['ci_95'][1]:.3f})\")\n",
    "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
    "print(f\"Unknown predictions: {results['unknown']}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "predictions = evaluate_qa_batch(prompts['golden_context'], ground_truths, llm, sampling_params, batch_size=32)\n",
    "results = calculate_metrics_with_ci(predictions, ground_truths)\n",
    "print(f\"Accuracy: {results['accuracy']:.3f}\")\n",
    "print(f\"Standard Deviation: {results['std']:.3f}\")\n",
    "print(f\"95% CI: ({results['ci_95'][0]:.3f}, {results['ci_95'][1]:.3f})\")\n",
    "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
    "print(f\"Unknown predictions: {results['unknown']}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "predictions = evaluate_qa_batch(prompts['retrieved_context'], ground_truths, llm, sampling_params, batch_size=4) # batch size reduced as context is longer\n",
    "results = calculate_metrics_with_ci(predictions, ground_truths)\n",
    "print(f\"Accuracy: {results['accuracy']:.3f}\")\n",
    "print(f\"Standard Deviation: {results['std']:.3f}\")\n",
    "print(f\"95% CI: ({results['ci_95'][0]:.3f}, {results['ci_95'][1]:.3f})\")\n",
    "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
    "print(f\"Unknown predictions: {results['unknown']}\")\n",
    "# ---------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
